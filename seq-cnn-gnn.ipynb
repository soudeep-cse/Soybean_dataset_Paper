{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":6161672,"datasetId":3534886,"databundleVersionId":6240662}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow\n!pip install scikit-learn\n!pip install torch torchvision\n!pip install torch-geometric\n!pip install optuna\n!pip install scikit-learn\n!pip install torch torchvision torchaudio\n!pip install torch-geometric\n!pip install timm\n# # or for huggingface transformers if you'd like to use that:\n!pip install transformers\n!pip install scikit-learn\n!pip install matplotlib opencv-python\n!pip install tensorflow\n!pip install keras-tuner","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"sivm205/soybean-diseased-leaf-dataset\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:29:17.411003Z","iopub.execute_input":"2025-02-10T18:29:17.411319Z","iopub.status.idle":"2025-02-10T18:29:19.379831Z","shell.execute_reply.started":"2025-02-10T18:29:17.411264Z","shell.execute_reply":"2025-02-10T18:29:19.379087Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/soybean-diseased-leaf-dataset\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, Subset\n\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n\n# Import PyTorch Geometric modules\nfrom torch_geometric.nn import GCNConv, global_mean_pool\n\nimport optuna\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:03:17.885507Z","iopub.execute_input":"2025-02-10T15:03:17.885837Z","iopub.status.idle":"2025-02-10T15:03:18.184524Z","shell.execute_reply.started":"2025-02-10T15:03:17.885818Z","shell.execute_reply":"2025-02-10T15:03:18.183869Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.layers as layers\nimport numpy as np\nimport networkx as nx\nimport scipy.sparse as sp\nimport os\nimport cv2\nfrom tensorflow.keras.applications import MobileNetV2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport optuna\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Dataset Directory\ndataset_dir = \"/kaggle/input/soybean-diseased-leaf-dataset\"\n\n# Load dataset images and labels\ndef load_dataset(dataset_dir, img_size=(224, 224)):\n    images = []\n    labels = []\n    class_names = sorted(os.listdir(dataset_dir))\n    class_dict = {class_name: idx for idx, class_name in enumerate(class_names)}\n    \n    for class_name in class_names:\n        class_path = os.path.join(dataset_dir, class_name)\n        for img_name in os.listdir(class_path):\n            img_path = os.path.join(class_path, img_name)\n            img = cv2.imread(img_path)\n            img = cv2.resize(img, img_size)\n            img = img / 255.0  # Normalize image\n            images.append(img)\n            labels.append(class_dict[class_name])\n    \n    return np.array(images), np.array(labels)\n\n# Load actual dataset\nimages, labels = load_dataset(dataset_dir)\n\n# Data Preprocessing & Augmentation\ndef preprocess_data(X, y):\n    datagen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        horizontal_flip=True,\n        rescale=1./255\n    )\n    return datagen.flow(X, y, batch_size=32)\n\n# Load MobileNetV2 as feature extractor\ndef create_cnn(input_shape):\n    base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n    base_model.trainable = False\n    inputs = tf.keras.Input(shape=input_shape)\n    x = base_model(inputs)\n    x = layers.GlobalAveragePooling2D()(x)\n    outputs = layers.Dense(128, activation='relu')(x)\n    model = tf.keras.Model(inputs, outputs)\n    return model\n\n# Extract features from images using MobileNetV2\ndef extract_features(images):\n    cnn_model = create_cnn((224, 224, 3))\n    batch_size = 16  # Reduce batch size to prevent memory overflow\n    return cnn_model.predict(images, batch_size=batch_size)\n\n# Construct graph using cosine similarity of image features\ndef create_graph(image_features, threshold=0.8):\n    similarity_matrix = cosine_similarity(image_features)\n    adj_matrix = (similarity_matrix > threshold).astype(int)\n    return sp.coo_matrix(adj_matrix)\n\n# Graph Convolutional Network (GCN) Layer\ndef create_gcn(adj_matrix, features):\n    input_features = tf.keras.Input(shape=(features.shape[1],))\n    x = layers.Dense(64, activation='relu')(input_features)\n    x = layers.Dense(32, activation='relu')(x)\n    outputs = layers.Dense(10, activation='softmax')(x)  # Output layer for classification\n    return tf.keras.Model(inputs=input_features, outputs=outputs)\n\n# Sequential CNN-GCN Model\ndef create_sequential_cnn_gnn(input_shape, adj_matrix):\n    cnn_model = create_cnn(input_shape)\n      # Free up GPU memory\n    cnn_output = cnn_model.output  # CNN extracts features first\n    \n    # Generate adjacency matrix and pass features to GCN\n    \n    gcn_model = create_gcn(adj_matrix, node_features)\n    \n    model = tf.keras.Model(inputs=cnn_model.input, outputs=gcn_model(cnn_output))\n    return model\n\n# Extract features and create graph\nnode_features = extract_features(images)\nadj_matrix = create_graph(node_features)\n\n# Train-Test Split\ntrain_data, test_data, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n\n# Create and compile the model\nsequential_model = create_sequential_cnn_gnn((224, 224, 3), adj_matrix)\nsequential_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nsequential_model.fit(train_data, tf.keras.utils.to_categorical(train_labels, num_classes=10), epochs=20, batch_size=32, validation_data=(test_data, tf.keras.utils.to_categorical(test_labels, num_classes=10)))\n\n# Evaluate the model\ndef evaluate_model(model, test_data, test_labels):\n    test_labels = np.argmax(test_labels, axis=1)  # Convert one-hot encoding to categorical labels\n    predictions = model.predict(test_data)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    acc = accuracy_score(test_labels, predicted_labels)\n    precision = precision_score(test_labels, predicted_labels, average='weighted')\n    recall = recall_score(test_labels, predicted_labels, average='weighted')\n    f1 = f1_score(test_labels, predicted_labels, average='weighted')\n    \n    print(f\"Accuracy: {acc:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    \n    return acc, precision, recall, f1\n\n# Evaluate and save the model\nevaluate_model(sequential_model, test_data, tf.keras.utils.to_categorical(test_labels, num_classes=10))\nsequential_model.save('sequential_cnn_gnn.h5')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:13:00.238185Z","iopub.execute_input":"2025-02-10T19:13:00.238548Z","iopub.status.idle":"2025-02-10T19:14:58.967710Z","shell.execute_reply.started":"2025-02-10T19:13:00.238522Z","shell.execute_reply":"2025-02-10T19:14:58.967037Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 64ms/step\nEpoch 1/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 532ms/step - accuracy: 0.4212 - loss: 1.7785 - val_accuracy: 0.8794 - val_loss: 0.5699\nEpoch 2/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.9106 - loss: 0.4581 - val_accuracy: 0.9149 - val_loss: 0.2917\nEpoch 3/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9436 - loss: 0.1829 - val_accuracy: 0.9362 - val_loss: 0.1943\nEpoch 4/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9755 - loss: 0.1116 - val_accuracy: 0.9504 - val_loss: 0.1683\nEpoch 5/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9726 - loss: 0.0849 - val_accuracy: 0.9433 - val_loss: 0.1612\nEpoch 6/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9878 - loss: 0.0540 - val_accuracy: 0.9574 - val_loss: 0.1249\nEpoch 7/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9933 - loss: 0.0522 - val_accuracy: 0.9504 - val_loss: 0.1656\nEpoch 8/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9933 - loss: 0.0248 - val_accuracy: 0.9574 - val_loss: 0.1302\nEpoch 9/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9863 - loss: 0.0336 - val_accuracy: 0.9504 - val_loss: 0.1069\nEpoch 10/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9901 - loss: 0.0286 - val_accuracy: 0.9645 - val_loss: 0.1084\nEpoch 11/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9946 - loss: 0.0197 - val_accuracy: 0.9574 - val_loss: 0.1152\nEpoch 12/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9952 - loss: 0.0244 - val_accuracy: 0.9716 - val_loss: 0.1231\nEpoch 13/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0080 - val_accuracy: 0.9574 - val_loss: 0.1155\nEpoch 14/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.9645 - val_loss: 0.1251\nEpoch 15/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 0.9645 - val_loss: 0.1462\nEpoch 16/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.9645 - val_loss: 0.1632\nEpoch 17/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.9716 - val_loss: 0.1275\nEpoch 18/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.9645 - val_loss: 0.1517\nEpoch 19/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9645 - val_loss: 0.1389\nEpoch 20/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.9645 - val_loss: 0.1416\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 652ms/step\nAccuracy: 0.9645\nPrecision: 0.9707\nRecall: 0.9645\nF1 Score: 0.9614\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}