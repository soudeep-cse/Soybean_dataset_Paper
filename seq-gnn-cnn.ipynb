{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":6161672,"datasetId":3534886,"databundleVersionId":6240662}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow\n!pip install scikit-learn\n!pip install torch torchvision\n!pip install torch-geometric\n!pip install optuna\n!pip install scikit-learn\n!pip install torch torchvision torchaudio\n!pip install torch-geometric\n!pip install timm\n# # or for huggingface transformers if you'd like to use that:\n!pip install transformers\n!pip install scikit-learn\n!pip install matplotlib opencv-python\n!pip install tensorflow\n!pip install keras-tuner","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"sivm205/soybean-diseased-leaf-dataset\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:25:36.038457Z","iopub.execute_input":"2025-02-10T19:25:36.038770Z","iopub.status.idle":"2025-02-10T19:25:56.117427Z","shell.execute_reply.started":"2025-02-10T19:25:36.038740Z","shell.execute_reply":"2025-02-10T19:25:56.116649Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/soybean-diseased-leaf-dataset\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.layers as layers\nimport numpy as np\nimport networkx as nx\nimport scipy.sparse as sp\nimport os\nimport cv2\nfrom tensorflow.keras.applications import MobileNetV2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport optuna\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Dataset Directory\ndataset_dir = \"/kaggle/input/soybean-diseased-leaf-dataset\"\n\n# Load dataset images and labels\ndef load_dataset(dataset_dir, img_size=(224, 224)):\n    images = []\n    labels = []\n    class_names = sorted(os.listdir(dataset_dir))\n    class_dict = {class_name: idx for idx, class_name in enumerate(class_names)}\n    \n    for class_name in class_names:\n        class_path = os.path.join(dataset_dir, class_name)\n        for img_name in os.listdir(class_path):\n            img_path = os.path.join(class_path, img_name)\n            img = cv2.imread(img_path)\n            img = cv2.resize(img, img_size)\n            img = img / 255.0  # Normalize image\n            images.append(img)\n            labels.append(class_dict[class_name])\n    \n    return np.array(images), np.array(labels)\n\n# Load actual dataset\nimages, labels = load_dataset(dataset_dir)\n\n# Construct graph using cosine similarity of image features\ndef create_graph(image_features, threshold=0.8):\n    similarity_matrix = cosine_similarity(image_features)\n    adj_matrix = (similarity_matrix > threshold).astype(int)\n    return sp.coo_matrix(adj_matrix)\n\n# Graph Convolutional Network (GCN) Layer\ndef create_gcn(input_shape):\n    inputs = tf.keras.Input(shape=input_shape)\n    x = layers.Dense(64, activation='relu')(inputs)\n    x = layers.Dense(32, activation='relu')(x)\n    outputs = layers.Dense(128, activation='relu')(x)\n    return tf.keras.Model(inputs, outputs)\n\n# Load MobileNetV2 as feature extractor\ndef create_cnn(input_shape):\n    base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(10, activation='softmax')\n    ])\n    return model\n\n# Sequential GCN-CNN Model\ndef create_sequential_gnn_cnn(input_shape, num_nodes):\n    gcn_model = create_gcn((128,))\n    cnn_model = create_cnn((224, 224, 3))\n    \n    gcn_input = tf.keras.Input(shape=(128,))\n    gcn_output = gcn_model(gcn_input)\n    \n    cnn_input = tf.keras.Input(shape=(224, 224, 3))\n    cnn_output = cnn_model(cnn_input)\n    \n    model = tf.keras.Model(inputs=[gcn_input, cnn_input], outputs=cnn_output)\n    return model\n\n# Extract features and create graph\nnode_features = np.random.rand(len(images), 128)  # Placeholder node features\nadj_matrix = create_graph(node_features)\n\n# Train-Test Split\ntrain_data, test_data, train_labels, test_labels, train_node_features, test_node_features = train_test_split(images, labels, node_features, test_size=0.2, random_state=42)\n\n# Create and compile the model\nsequential_model = create_sequential_gnn_cnn((224, 224, 3), len(images))\nsequential_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nsequential_model.fit([train_node_features, train_data], tf.keras.utils.to_categorical(train_labels, num_classes=10), epochs=20, batch_size=32, validation_data=([test_node_features, test_data], tf.keras.utils.to_categorical(test_labels, num_classes=10)))\n\n# Evaluate the model\ndef evaluate_model(model, test_data, test_labels):\n    test_labels = np.argmax(test_labels, axis=1)  # Convert one-hot encoding to categorical labels\n    predictions = model.predict(test_data)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    acc = accuracy_score(test_labels, predicted_labels)\n    precision = precision_score(test_labels, predicted_labels, average='weighted')\n    recall = recall_score(test_labels, predicted_labels, average='weighted')\n    f1 = f1_score(test_labels, predicted_labels, average='weighted')\n    \n    print(f\"Accuracy: {acc:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    \n    return acc, precision, recall, f1\n\n# Evaluate and save the model\nevaluate_model(sequential_model, [test_node_features, test_data], tf.keras.utils.to_categorical(test_labels, num_classes=10))\nsequential_model.save('sequential_gnn_cnn.h5')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:56:24.444375Z","iopub.execute_input":"2025-02-10T19:56:24.444698Z","iopub.status.idle":"2025-02-10T19:58:12.300000Z","shell.execute_reply.started":"2025-02-10T19:56:24.444675Z","shell.execute_reply":"2025-02-10T19:58:12.299105Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 354ms/step - accuracy: 0.3440 - loss: 1.8981 - val_accuracy: 0.8014 - val_loss: 0.6286\nEpoch 2/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.8572 - loss: 0.4873 - val_accuracy: 0.9078 - val_loss: 0.3984\nEpoch 3/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9003 - loss: 0.3168 - val_accuracy: 0.9220 - val_loss: 0.3046\nEpoch 4/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9179 - loss: 0.2372 - val_accuracy: 0.9362 - val_loss: 0.2587\nEpoch 5/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9495 - loss: 0.1774 - val_accuracy: 0.9362 - val_loss: 0.2230\nEpoch 6/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9541 - loss: 0.1383 - val_accuracy: 0.9433 - val_loss: 0.2104\nEpoch 7/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9733 - loss: 0.1049 - val_accuracy: 0.9362 - val_loss: 0.2002\nEpoch 8/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9807 - loss: 0.1033 - val_accuracy: 0.9574 - val_loss: 0.1611\nEpoch 9/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9867 - loss: 0.0943 - val_accuracy: 0.9433 - val_loss: 0.1816\nEpoch 10/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9893 - loss: 0.0724 - val_accuracy: 0.9645 - val_loss: 0.1591\nEpoch 11/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9958 - loss: 0.0674 - val_accuracy: 0.9504 - val_loss: 0.1575\nEpoch 12/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9944 - loss: 0.0624 - val_accuracy: 0.9574 - val_loss: 0.1609\nEpoch 13/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9981 - loss: 0.0501 - val_accuracy: 0.9645 - val_loss: 0.1463\nEpoch 14/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9946 - loss: 0.0405 - val_accuracy: 0.9645 - val_loss: 0.1459\nEpoch 15/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9961 - loss: 0.0431 - val_accuracy: 0.9645 - val_loss: 0.1567\nEpoch 16/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9937 - loss: 0.0423 - val_accuracy: 0.9645 - val_loss: 0.1470\nEpoch 17/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9982 - loss: 0.0359 - val_accuracy: 0.9645 - val_loss: 0.1427\nEpoch 18/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0355 - val_accuracy: 0.9645 - val_loss: 0.1420\nEpoch 19/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9966 - loss: 0.0268 - val_accuracy: 0.9716 - val_loss: 0.1332\nEpoch 20/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9993 - loss: 0.0291 - val_accuracy: 0.9645 - val_loss: 0.1374\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 641ms/step\nAccuracy: 0.9645\nPrecision: 0.9707\nRecall: 0.9645\nF1 Score: 0.9567\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}