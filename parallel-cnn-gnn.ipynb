{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":6161672,"datasetId":3534886,"databundleVersionId":6240662}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow\n!pip install scikit-learn\n!pip install torch torchvision\n!pip install torch-geometric\n!pip install optuna\n!pip install scikit-learn\n!pip install torch torchvision torchaudio\n!pip install torch-geometric\n!pip install timm\n# # or for huggingface transformers if you'd like to use that:\n!pip install transformers\n!pip install scikit-learn\n!pip install matplotlib opencv-python\n!pip install tensorflow\n!pip install keras-tuner","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"sivm205/soybean-diseased-leaf-dataset\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:35:47.788783Z","iopub.execute_input":"2025-02-10T15:35:47.789134Z","iopub.status.idle":"2025-02-10T15:35:51.049208Z","shell.execute_reply.started":"2025-02-10T15:35:47.789106Z","shell.execute_reply":"2025-02-10T15:35:51.048541Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/soybean-diseased-leaf-dataset\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision import transforms, models\n\n# PyTorch Geometric imports for the GNN branch\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.nn import GCNConv, global_mean_pool\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:36:00.400313Z","iopub.execute_input":"2025-02-10T15:36:00.400720Z","iopub.status.idle":"2025-02-10T15:36:08.439607Z","shell.execute_reply.started":"2025-02-10T15:36:00.400696Z","shell.execute_reply":"2025-02-10T15:36:08.438865Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nParallel CNN+GNN Model for Image Classification\n\nThis script implements a parallel architecture in which an image is processed\nsimultaneously by a CNN branch (using MobileNetV2) and a GNN branch (using a GCN).\nThe CNN branch extracts spatial features from the image, while the GNN branch converts\nthe image into a graph (each patch becomes a node with the mean RGB value) and processes\nthe graph structure. The resulting features are fused and passed to a classification head.\n\nAuthor: [Your Name]\nDate: [Current Date]\n\"\"\"\n\n# -----------------------------\n# 1. Set Random Seeds for Reproducibility\n# -----------------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# -----------------------------\n# 2. Image-to-Graph Conversion Function\n# -----------------------------\ndef image_to_graph(image: np.ndarray, patch_size=(16, 16)) -> Data:\n    \"\"\"\n    Converts an image (H, W, C) into a graph (PyG Data object) by splitting it into patches.\n    \n    Each patch is represented by its mean RGB value (node feature). Nodes are connected\n    to their immediate 4-neighbors (up, down, left, right).\n\n    Args:\n        image (np.ndarray): Input image with shape [H, W, 3].\n        patch_size (tuple): Size (height, width) of each patch.\n\n    Returns:\n        Data: A PyTorch Geometric Data object with node features 'x' and edge_index.\n    \"\"\"\n    h, w, c = image.shape\n    ph, pw = patch_size\n    num_nodes_h = h // ph\n    num_nodes_w = w // pw\n    nodes = []\n\n    # Compute mean RGB for each patch\n    for i in range(num_nodes_h):\n        for j in range(num_nodes_w):\n            patch = image[i*ph:(i+1)*ph, j*pw:(j+1)*pw, :]\n            patch_feature = patch.mean(axis=(0, 1))\n            nodes.append(patch_feature)\n    nodes = np.array(nodes, dtype=np.float32)\n\n    # Build edge_index for 4-connected grid\n    edge_index = []\n    for i in range(num_nodes_h):\n        for j in range(num_nodes_w):\n            node_idx = i * num_nodes_w + j\n            # Up neighbor\n            if i > 0:\n                neighbor = (i - 1) * num_nodes_w + j\n                edge_index.append([node_idx, neighbor])\n                edge_index.append([neighbor, node_idx])\n            # Down neighbor\n            if i < num_nodes_h - 1:\n                neighbor = (i + 1) * num_nodes_w + j\n                edge_index.append([node_idx, neighbor])\n                edge_index.append([neighbor, node_idx])\n            # Left neighbor\n            if j > 0:\n                neighbor = i * num_nodes_w + (j - 1)\n                edge_index.append([node_idx, neighbor])\n                edge_index.append([neighbor, node_idx])\n            # Right neighbor\n            if j < num_nodes_w - 1:\n                neighbor = i * num_nodes_w + (j + 1)\n                edge_index.append([node_idx, neighbor])\n                edge_index.append([neighbor, node_idx])\n    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n\n    # Create the Data object\n    x = torch.tensor(nodes, dtype=torch.float)\n    data = Data(x=x, edge_index=edge_index)\n    return data\n\n# -----------------------------\n# 3. Custom Dataset for Parallel CNN+GNN\n# -----------------------------\nclass ParallelCnnGnnDataset(Dataset):\n    \"\"\"\n    Dataset that returns a tuple for each image:\n      - A CNN-processed image tensor.\n      - A graph (PyG Data object) constructed from the image.\n      - The class label.\n      \n    The dataset assumes a folder structure:\n        root_dir/\n            class_0/\n                img1.jpg, img2.jpg, ...\n            class_1/\n                ...\n    \"\"\"\n    def __init__(self, root_dir, cnn_transform=None, graph_patch_size=(16, 16), image_size=(224, 224)):\n        \"\"\"\n        Args:\n            root_dir (str): Path to the dataset directory.\n            cnn_transform: Transformations for the CNN branch (e.g., Resize, ToTensor, Normalize).\n            graph_patch_size (tuple): Patch size for graph conversion.\n            image_size (tuple): Final image size for CNN input.\n        \"\"\"\n        self.root_dir = root_dir\n        self.cnn_transform = cnn_transform\n        self.graph_patch_size = graph_patch_size\n        self.image_size = image_size\n        \n        # List all subdirectories as classes (sorted for consistency)\n        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n        \n        # Build list of (image_path, label) tuples\n        self.samples = []\n        for cls in self.classes:\n            cls_path = os.path.join(root_dir, cls)\n            for fname in os.listdir(cls_path):\n                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    self.samples.append((os.path.join(cls_path, fname), self.class_to_idx[cls]))\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        path, label = self.samples[idx]\n        # Open image and ensure RGB\n        with Image.open(path) as img:\n            img = img.convert('RGB')\n            # For CNN branch: apply transformation if provided\n            if self.cnn_transform:\n                cnn_img = self.cnn_transform(img)\n            else:\n                img_resized = img.resize(self.image_size)\n                cnn_img = transforms.ToTensor()(img_resized)\n            \n            # For GNN branch: resize (if needed) and convert to numpy array\n            img_for_graph = img.resize(self.image_size)\n            img_np = np.array(img_for_graph)\n        \n        # Convert image to graph\n        graph_data = image_to_graph(img_np, patch_size=self.graph_patch_size)\n        graph_data.y = torch.tensor([label], dtype=torch.long)\n        \n        return cnn_img, graph_data, label\n\n# -----------------------------\n# 4. Custom Collate Function for DataLoader\n# -----------------------------\ndef parallel_collate_fn(batch):\n    \"\"\"\n    Collate function to merge a list of samples into a batch.\n    \n    Each sample is a tuple (cnn_img, graph_data, label). This function stacks the CNN images\n    into a tensor and batches the graph objects using PyG's Batch.from_data_list.\n    \"\"\"\n    cnn_imgs, graph_data_list, labels = [], [], []\n    for cnn_img, g_data, label in batch:\n        cnn_imgs.append(cnn_img)\n        graph_data_list.append(g_data)\n        labels.append(label)\n    \n    cnn_batch = torch.stack(cnn_imgs, dim=0)\n    gnn_batch = Batch.from_data_list(graph_data_list)\n    labels = torch.tensor(labels, dtype=torch.long)\n    return cnn_batch, gnn_batch, labels\n\n# -----------------------------\n# 5. Parallel CNN+GNN Model Definition\n# -----------------------------\nclass ParallelCNNGNN(nn.Module):\n    \"\"\"\n    A parallel model that fuses features from a CNN branch (MobileNetV2) and a GNN branch.\n    \n    The CNN branch extracts features from the image, while the GNN branch extracts graph-level\n    features from a patch-based representation of the same image. The features are concatenated\n    and fed to a final classifier.\n    \"\"\"\n    def __init__(self, num_classes, gnn_num_layers=2, gnn_hidden_dim=64, gnn_dropout=0.5, freeze_cnn=False):\n        \"\"\"\n        Args:\n            num_classes (int): Number of classes.\n            gnn_num_layers (int): Number of GCNConv layers.\n            gnn_hidden_dim (int): Hidden dimension for GNN layers.\n            gnn_dropout (float): Dropout probability for the GNN branch.\n            freeze_cnn (bool): If True, freezes the CNN branch.\n        \"\"\"\n        super(ParallelCNNGNN, self).__init__()\n        \n        # --- CNN Branch: MobileNetV2 ---\n        # Load pretrained MobileNetV2 (with weights on ImageNet)\n        self.cnn = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n        # Remove the classifier head; we use only the feature extractor.\n        self.cnn.classifier = nn.Identity()\n        if freeze_cnn:\n            for param in self.cnn.parameters():\n                param.requires_grad = False\n        \n        # Global pooling to reduce CNN feature maps to a vector.\n        self.global_pool_cnn = nn.AdaptiveAvgPool2d((1, 1))\n        \n        # --- GNN Branch ---\n        # For image-to-graph conversion, the node features are of size 3 (mean RGB).\n        in_feats = 3\n        self.convs = nn.ModuleList()\n        self.convs.append(GCNConv(in_feats, gnn_hidden_dim))\n        for _ in range(gnn_num_layers - 1):\n            self.convs.append(GCNConv(gnn_hidden_dim, gnn_hidden_dim))\n        self.gnn_dropout = gnn_dropout\n        \n        # --- Fusion and Final Classification ---\n        # CNN branch outputs 1280 features (for MobileNetV2) and GNN branch outputs gnn_hidden_dim.\n        self.fc_fusion = nn.Linear(1280 + gnn_hidden_dim, num_classes)\n    \n    def forward(self, cnn_batch, gnn_batch):\n        # ----- CNN Branch -----\n        # Extract features using the MobileNetV2 feature extractor.\n        x_cnn = self.cnn.features(cnn_batch)          # [B, 1280, H_feat, W_feat]\n        x_cnn = self.global_pool_cnn(x_cnn)             # [B, 1280, 1, 1]\n        x_cnn = x_cnn.view(x_cnn.size(0), -1)           # [B, 1280]\n        \n        # ----- GNN Branch -----\n        x_gnn = gnn_batch.x                           # Node features: [total_nodes, 3]\n        edge_index = gnn_batch.edge_index             # Edge indices\n        batch_idx = gnn_batch.batch                   # Batch vector mapping nodes to graphs\n        \n        # Pass through each GCN layer with ReLU activation and dropout.\n        for conv in self.convs:\n            x_gnn = conv(x_gnn, edge_index)\n            x_gnn = F.relu(x_gnn)\n            x_gnn = F.dropout(x_gnn, p=self.gnn_dropout, training=self.training)\n        \n        # Global pooling to obtain graph-level features.\n        x_gnn = global_mean_pool(x_gnn, batch_idx)      # [B, gnn_hidden_dim]\n        \n        # ----- Fusion -----\n        x_fused = torch.cat([x_cnn, x_gnn], dim=1)       # [B, 1280 + gnn_hidden_dim]\n        logits = self.fc_fusion(x_fused)                # [B, num_classes]\n        return logits\n\n# -----------------------------\n# 6. Training and Evaluation Routine\n# -----------------------------\ndef train_parallel_cnn_gnn():\n    # ----------- Configuration -----------\n    data_dir = \"/kaggle/input/soybean-diseased-leaf-dataset\"  # Update path as needed\n    batch_size = 8\n    num_epochs = 10\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Determine the number of classes based on subdirectories in the dataset\n    classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n    num_classes = len(classes)\n    print(f\"Detected {num_classes} classes: {classes}\")\n    \n    # ----------- Transforms for CNN Branch -----------\n    cnn_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225]),\n    ])\n    \n    # ----------- Create Dataset and DataLoaders -----------\n    dataset = ParallelCnnGnnDataset(root_dir=data_dir,\n                                    cnn_transform=cnn_transform,\n                                    graph_patch_size=(16, 16),\n                                    image_size=(224, 224))\n    # Split dataset: 80% training, 20% validation\n    total_samples = len(dataset)\n    indices = list(range(total_samples))\n    split = int(0.2 * total_samples)\n    val_indices = indices[:split]\n    train_indices = indices[split:]\n    \n    train_dataset = Subset(dataset, train_indices)\n    val_dataset   = Subset(dataset, val_indices)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                              collate_fn=parallel_collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n                            collate_fn=parallel_collate_fn)\n    \n    # ----------- Initialize Model, Loss, and Optimizer -----------\n    model = ParallelCNNGNN(num_classes=num_classes,\n                           gnn_num_layers=2,\n                           gnn_hidden_dim=64,\n                           gnn_dropout=0.5,\n                           freeze_cnn=False)\n    model.to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    \n    # ----------- Training Loop -----------\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0.0\n        for cnn_batch, gnn_batch, labels in train_loader:\n            cnn_batch = cnn_batch.to(device)\n            gnn_batch = gnn_batch.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(cnn_batch, gnn_batch)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item() * cnn_batch.size(0)\n        \n        avg_loss = total_loss / len(train_loader.dataset)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_loss:.4f}\")\n        \n        # ----------- Validation -----------\n        model.eval()\n        val_preds = []\n        val_labels = []\n        with torch.no_grad():\n            for cnn_batch, gnn_batch, labels in val_loader:\n                cnn_batch = cnn_batch.to(device)\n                gnn_batch = gnn_batch.to(device)\n                labels = labels.to(device)\n                outputs = model(cnn_batch, gnn_batch)\n                _, preds = torch.max(outputs, 1)\n                val_preds.extend(preds.cpu().numpy())\n                val_labels.extend(labels.cpu().numpy())\n        \n        val_acc = accuracy_score(val_labels, val_preds)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] Validation Accuracy: {val_acc:.4f}\")\n    \n    # ----------- Final Evaluation on Validation Set -----------\n    prec = precision_score(val_labels, val_preds, average='weighted', zero_division=0)\n    rec  = recall_score(val_labels, val_preds, average='weighted', zero_division=0)\n    f1   = f1_score(val_labels, val_preds, average='weighted', zero_division=0)\n    \n    print(\"\\nFinal Evaluation Metrics on Validation Set:\")\n    print(f\"Accuracy:  {val_acc:.4f}\")\n    print(f\"Precision: {prec:.4f}\")\n    print(f\"Recall:    {rec:.4f}\")\n    print(f\"F1 Score:  {f1:.4f}\\n\")\n    print(\"Classification Report:\")\n    print(classification_report(val_labels, val_preds, zero_division=0))\n\n# -----------------------------\n# 7. Main Execution\n# -----------------------------\nif __name__ == '__main__':\n    train_parallel_cnn_gnn()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:36:30.716682Z","iopub.execute_input":"2025-02-10T15:36:30.717204Z","iopub.status.idle":"2025-02-10T16:03:08.421953Z","shell.execute_reply.started":"2025-02-10T15:36:30.717175Z","shell.execute_reply":"2025-02-10T16:03:08.421038Z"}},"outputs":[{"name":"stdout","text":"Detected 10 classes: ['Mossaic Virus', 'Southern blight', 'Sudden Death Syndrone', 'Yellow Mosaic', 'bacterial_blight', 'brown_spot', 'crestamento', 'ferrugen', 'powdery_mildew', 'septoria']\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n100%|██████████| 13.6M/13.6M [00:00<00:00, 110MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10] Training Loss: 0.9164\nEpoch [1/10] Validation Accuracy: 0.3058\nEpoch [2/10] Training Loss: 0.1864\nEpoch [2/10] Validation Accuracy: 0.3058\nEpoch [3/10] Training Loss: 0.1286\nEpoch [3/10] Validation Accuracy: 0.3058\nEpoch [4/10] Training Loss: 0.0501\nEpoch [4/10] Validation Accuracy: 0.3058\nEpoch [5/10] Training Loss: 0.0525\nEpoch [5/10] Validation Accuracy: 0.3058\nEpoch [6/10] Training Loss: 0.0343\nEpoch [6/10] Validation Accuracy: 0.3058\nEpoch [7/10] Training Loss: 0.0595\nEpoch [7/10] Validation Accuracy: 0.3058\nEpoch [8/10] Training Loss: 0.0603\nEpoch [8/10] Validation Accuracy: 0.3058\nEpoch [9/10] Training Loss: 0.0146\nEpoch [9/10] Validation Accuracy: 0.3058\nEpoch [10/10] Training Loss: 0.0165\nEpoch [10/10] Validation Accuracy: 0.3058\n\nFinal Evaluation Metrics on Validation Set:\nAccuracy:  0.3058\nPrecision: 0.3058\nRecall:    0.3058\nF1 Score:  0.3058\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        22\n           1       0.00      0.00      0.00        62\n           2       1.00      1.00      1.00        37\n           3       0.00      0.00      0.00         0\n           7       0.00      0.00      0.00         0\n           9       0.00      0.00      0.00         0\n\n    accuracy                           0.31       121\n   macro avg       0.17      0.17      0.17       121\nweighted avg       0.31      0.31      0.31       121\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Working with New Parallel CNN and GNN","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow numpy networkx scipy opencv-python scikit-learn optuna","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.layers as layers\nimport numpy as np\nimport networkx as nx\nimport scipy.sparse as sp\nimport os\nimport cv2\nfrom tensorflow.keras.applications import MobileNetV2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport optuna\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset Directory\ndataset_dir = \"/kaggle/input/soybean-diseased-leaf-dataset\"\n\n# Load dataset images and labels\ndef load_dataset(dataset_dir, img_size=(224, 224)):\n    images = []\n    labels = []\n    class_names = sorted(os.listdir(dataset_dir))\n    class_dict = {class_name: idx for idx, class_name in enumerate(class_names)}\n    \n    for class_name in class_names:\n        class_path = os.path.join(dataset_dir, class_name)\n        for img_name in os.listdir(class_path):\n            img_path = os.path.join(class_path, img_name)\n            img = cv2.imread(img_path)\n            img = cv2.resize(img, img_size)\n            img = img / 255.0  # Normalize image\n            images.append(img)\n            labels.append(class_dict[class_name])\n    \n    return np.array(images), np.array(labels)\n\n# Load actual dataset\nimages, labels = load_dataset(dataset_dir)\n\n# Data Preprocessing & Augmentation\ndef preprocess_data(X, y):\n    datagen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        horizontal_flip=True,\n        rescale=1./255\n    )\n    return datagen.flow(X, y, batch_size=32)\n\n# Load MobileNetV2 as feature extractor\ndef create_cnn(input_shape):\n    base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(128, activation='relu')\n    ])\n    return model\n\n# Extract features from images using MobileNetV2\ndef extract_features(images):\n    cnn_model = create_cnn((224, 224, 3))\n    return cnn_model.predict(images)\n\n# Construct graph using cosine similarity of image features\ndef create_graph(image_features, threshold=0.8):\n    similarity_matrix = cosine_similarity(image_features)\n    adj_matrix = (similarity_matrix > threshold).astype(int)\n    return sp.coo_matrix(adj_matrix)\n\n# Graph Convolutional Network (GCN) Layer\ndef create_gcn(adj_matrix, features):\n    input_features = tf.keras.Input(shape=(features.shape[1],))\n    x = layers.Dense(64, activation='relu')(input_features)\n    x = layers.Dense(32, activation='relu')(x)\n    output = layers.Dense(128, activation='relu')(x)\n    return tf.keras.Model(inputs=input_features, outputs=output)\n\n# Hybrid Model Combining CNN & GCN\ndef create_hybrid_model(input_shape, adj_matrix, node_features):\n    cnn_model = create_cnn(input_shape)\n    gcn_model = create_gcn(adj_matrix, node_features)\n    \n    cnn_input = tf.keras.Input(shape=input_shape)\n    gcn_input = tf.keras.Input(shape=(node_features.shape[1],))\n    \n    cnn_output = cnn_model(cnn_input)\n    gcn_output = gcn_model(gcn_input)\n    \n    merged = layers.Concatenate()([cnn_output, gcn_output])\n    final_output = layers.Dense(10, activation='softmax')(merged)\n    \n    model = tf.keras.Model(inputs=[cnn_input, gcn_input], outputs=final_output)\n    return model\n\n# Evaluate model performance\ndef evaluate_model(model, test_data, test_labels):\n    test_labels = np.argmax(test_labels, axis=1)  # Convert one-hot encoding to categorical labels\n    \n    predictions = model.predict(test_data)\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    acc = accuracy_score(test_labels, predicted_labels)\n    precision = precision_score(test_labels, predicted_labels, average='weighted')\n    recall = recall_score(test_labels, predicted_labels, average='weighted')\n    f1 = f1_score(test_labels, predicted_labels, average='weighted')\n\n    print(f\"Accuracy: {acc:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n\n    return acc, precision, recall, f1\n\n# Define objective function for Optuna\ndef objective(trial):\n    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n    \n    model = create_hybrid_model((224, 224, 3), adj_matrix, node_features)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    history = model.fit([train_data, node_features], tf.keras.utils.to_categorical(train_labels, num_classes=10), epochs=5, batch_size=batch_size, validation_split=0.2, verbose=0)\n    return max(history.history['val_accuracy'])\n\n# Extract features and create graph\nnode_features = extract_features(images)\nadj_matrix = create_graph(node_features)\n\n# Train-Test Split\ntrain_data, test_data, train_labels, test_labels, train_node_features, test_node_features = train_test_split(images, labels, node_features, test_size=0.2, random_state=42)\n\n# Run Hyperparameter Optimization\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\n\n# Train Final Model with Best Parameters\nbest_params = study.best_params\nfinal_model = create_hybrid_model((224, 224, 3), adj_matrix, node_features)\nfinal_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['lr']), loss='categorical_crossentropy', metrics=['accuracy'])\nfinal_model.fit([train_data, train_node_features], tf.keras.utils.to_categorical(train_labels, num_classes=10), epochs=20, batch_size=best_params['batch_size'], validation_data=([test_data, test_node_features], tf.keras.utils.to_categorical(test_labels, num_classes=10)))\n\n# Evaluate and Save Model\nevaluate_model(final_model, [test_data, test_node_features], tf.keras.utils.to_categorical(test_labels, num_classes=10))\nfinal_model.save('hybrid_model.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T17:25:17.101560Z","iopub.execute_input":"2025-02-10T17:25:17.101996Z","iopub.status.idle":"2025-02-10T17:29:45.550263Z","shell.execute_reply.started":"2025-02-10T17:25:17.101965Z","shell.execute_reply":"2025-02-10T17:29:45.549484Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 122ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-10 17:26:36,153] A new study created in memory with name: no-name-a83d09e1-6ee7-48cb-a00e-de2f9eddf190\n[I 2025-02-10 17:26:50,910] Trial 0 finished with value: 0.4732142984867096 and parameters: {'lr': 3.3670945851963566e-05, 'batch_size': 64}. Best is trial 0 with value: 0.4732142984867096.\n[I 2025-02-10 17:27:07,994] Trial 1 finished with value: 0.2410714328289032 and parameters: {'lr': 1.2513966301630107e-05, 'batch_size': 64}. Best is trial 0 with value: 0.4732142984867096.\n[I 2025-02-10 17:27:22,994] Trial 2 finished with value: 0.4375 and parameters: {'lr': 2.2128817338785996e-05, 'batch_size': 64}. Best is trial 0 with value: 0.4732142984867096.\n[I 2025-02-10 17:27:38,338] Trial 3 finished with value: 0.8035714030265808 and parameters: {'lr': 0.00011781646662243641, 'batch_size': 64}. Best is trial 3 with value: 0.8035714030265808.\n[I 2025-02-10 17:27:53,550] Trial 4 finished with value: 0.4732142984867096 and parameters: {'lr': 4.001247355729215e-05, 'batch_size': 64}. Best is trial 3 with value: 0.8035714030265808.\n[I 2025-02-10 17:28:08,562] Trial 5 finished with value: 0.9642857313156128 and parameters: {'lr': 0.0039056194109604436, 'batch_size': 32}. Best is trial 5 with value: 0.9642857313156128.\n[I 2025-02-10 17:28:23,241] Trial 6 finished with value: 0.9375 and parameters: {'lr': 0.0005401846659304643, 'batch_size': 16}. Best is trial 5 with value: 0.9642857313156128.\n[I 2025-02-10 17:28:38,364] Trial 7 finished with value: 0.9464285969734192 and parameters: {'lr': 0.0010027417807820082, 'batch_size': 32}. Best is trial 5 with value: 0.9642857313156128.\n[I 2025-02-10 17:28:56,353] Trial 8 finished with value: 0.75 and parameters: {'lr': 3.3021019797609015e-05, 'batch_size': 32}. Best is trial 5 with value: 0.9642857313156128.\n[I 2025-02-10 17:29:11,329] Trial 9 finished with value: 0.9642857313156128 and parameters: {'lr': 0.004933965663055586, 'batch_size': 64}. Best is trial 5 with value: 0.9642857313156128.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 404ms/step - accuracy: 0.6504 - loss: 1.1572 - val_accuracy: 0.9504 - val_loss: 0.2260\nEpoch 2/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.9746 - loss: 0.1134 - val_accuracy: 0.9291 - val_loss: 0.1943\nEpoch 3/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9892 - loss: 0.0642 - val_accuracy: 0.9078 - val_loss: 0.2660\nEpoch 4/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9723 - loss: 0.0938 - val_accuracy: 0.9504 - val_loss: 0.2291\nEpoch 5/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9923 - loss: 0.0258 - val_accuracy: 0.9574 - val_loss: 0.1383\nEpoch 6/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9977 - loss: 0.0143 - val_accuracy: 0.9645 - val_loss: 0.1309\nEpoch 7/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9935 - loss: 0.0208 - val_accuracy: 0.9504 - val_loss: 0.1266\nEpoch 8/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9942 - loss: 0.0105 - val_accuracy: 0.9645 - val_loss: 0.1915\nEpoch 9/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.9574 - val_loss: 0.1481\nEpoch 10/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.9574 - val_loss: 0.1495\nEpoch 11/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.9574 - val_loss: 0.1542\nEpoch 12/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9574 - val_loss: 0.1702\nEpoch 13/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 8.9188e-04 - val_accuracy: 0.9574 - val_loss: 0.1850\nEpoch 14/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 7.3325e-04 - val_accuracy: 0.9574 - val_loss: 0.1717\nEpoch 15/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 6.9279e-04 - val_accuracy: 0.9574 - val_loss: 0.1874\nEpoch 16/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 5.8570e-04 - val_accuracy: 0.9574 - val_loss: 0.1939\nEpoch 17/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 4.4913e-04 - val_accuracy: 0.9574 - val_loss: 0.1956\nEpoch 18/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 5.2511e-04 - val_accuracy: 0.9574 - val_loss: 0.2118\nEpoch 19/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.7439e-04 - val_accuracy: 0.9574 - val_loss: 0.2110\nEpoch 20/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 4.1607e-04 - val_accuracy: 0.9574 - val_loss: 0.2222\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 565ms/step\nAccuracy: 0.9574\nPrecision: 0.9636\nRecall: 0.9574\nF1 Score: 0.9496\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}